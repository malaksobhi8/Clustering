# -*- coding: utf-8 -*-
"""DM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PT1J9I1Oot0OiEgawtZdtv6eGS3UGXC5
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn_extra.cluster import KMedoids
from sklearn.cluster import DBSCAN, KMeans
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
from random import sample
from numpy.random import uniform
from math import isnan
from sklearn.preprocessing import LabelEncoder

data = pd.read_csv("/content/Mall_Customers.csv")

data.head(10)

data.info()

data.describe()

# Define quartiles for 'Annual Income (k$)'
quartiles = [0, 41.5, 61.5, 78, 137]

# Labels for income ranges
labels = ['<$41.5k', '$41.5k-$61.5k', '$61.5k-$78k', '>$78k']

# Create 'Income Range' column based on quartiles
data['Income Range'] = pd.cut(data['Annual Income (k$)'], bins=quartiles, labels=labels)

# Group by 'Gender' and 'Income Range', and count occurrences
income_range_counts = data.groupby(['Gender', 'Income Range']).size()

# Print the result
print(income_range_counts)

sns.distplot(data['Annual Income (k$)'])
plt.title('Distribution of Annual Income', fontsize = 20)
plt.xlabel('Range of Annual Income')
plt.ylabel('Count')
plt.show()

sns.distplot(data['Age'])
plt.title('Distribution of Age', fontsize = 20)
plt.xlabel('Range of Age')
plt.ylabel('Count')
plt.show()

x = data['Annual Income (k$)']
y = data['Age']

plt.plot(x, y)

x = data['Annual Income (k$)']
y = data['Spending Score (1-100)']

plt.plot(x, y)

label_encoder = LabelEncoder()
data['Age'] = label_encoder.fit_transform(data['Age'].astype(str))
male = data[data['Gender'] == 'Male']
female = data[data['Gender'] == 'Female']
plt.figure(figsize=(8, 6))
sns.scatterplot(x="Spending Score (1-100)", y="Age", data=male, label='Male')
sns.scatterplot(x="Spending Score (1-100)", y="Age", data=female, label='Female')
plt.title('Spending Score by Geaqnder')
plt.xlabel('Spending Score (1-100)')
plt.ylabel('Age')
plt.legend()
plt.show()

# Function to calculate Hopkins statistic
def hopkins(X):
    d = X.shape[1]
    n = len(X)
    m = int(0.1 * n)
    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)

    rand_X = sample(range(0, n, 1), m)

    ujd = []
    wjd = []
    for j in range(0, m):
        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)
        ujd.append(u_dist[0][1])
        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)
        wjd.append(w_dist[0][1])

    H = sum(ujd) / (sum(ujd) + sum(wjd))
    if isnan(H):
        print(ujd, wjd)
        H = 0

    return H

# Compute Hopkins statistic
hopkins_statistic = hopkins(data[['Annual Income (k$)', 'Spending Score (1-100)']])
print("Hopkins Statistic:", hopkins_statistic)
#Hopkins Statistic Calculation:
#The hopkins() function calculates the Hopkins statistic, which measures the cluster tendency of the dataset.
# A value close to 1 suggests high cluster tendency.

"""#The output:    
#Hopkins Statistic: 0.8000357686324077
a Hopkins statistic of approximately 0.80 indicates that the dataset has a strong tendency to form clusters based on the  #features used for clustering (Annual Income and Spending Score). This implies that the data points are not uniformly distributed
across the feature space but rather tend to be closer to other points within the same cluster and farther away from poits in other
clusters.
"""

# K-Medoids clustering
kmedoids = KMedoids(n_clusters=5, random_state=0)
data['KMedoids_Clusters'] = kmedoids.fit_predict(data[['Annual Income (k$)','Spending Score (1-100)']])

# DBSCAN clustering
dbscan = DBSCAN(eps=3, min_samples=2)
data['DBSCAN_Clusters'] = dbscan.fit_predict(data[['Annual Income (k$)','Spending Score (1-100)']])

# Elbow Method for K-Means
X = data[['Annual Income (k$)', 'Spending Score (1-100)']]
wss = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    wss.append(kmeans.inertia_)

# Plot Elbow Method
plt.figure(figsize=(12, 5))
sns.lineplot(x=K, y=wss, marker='o')
plt.title('Elbow Method for K-Means')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Within-Cluster-Sum of Squared Errors (WSS)')
plt.show()

# Plotting K-Medoids clusters
sns.scatterplot(x="Spending Score (1-100)", y="Annual Income (k$)", hue='KMedoids_Clusters', data=data, palette='viridis')
plt.title('K-Medoids Clustering')
plt.show()

# Plotting DBSCAN clusters
sns.scatterplot(x="Spending Score (1-100)", y="Annual Income (k$)", hue='DBSCAN_Clusters', data=data, palette='viridis')
plt.title('DBSCAN Clustering')
plt.tight_layout()
plt.show()

# Compute silhouette score for KMedoids clustering
silhouette_kmedoids = silhouette_score(X, data['KMedoids_Clusters'])
print("Silhouette Score for K-Medoids Clustering:", silhouette_kmedoids)

# Compute silhouette score for DBSCAN clustering
silhouette_dbscan = silhouette_score(X, data['DBSCAN_Clusters'])
print("Silhouette Score for DBSCAN Clustering:", silhouette_dbscan)

"""explaining the two outputs:
In summary, the silhouette scores indicate that the K-Medoids clustering has produced clusters that are more well-defined
and separated compared to the DBSCAN clustering. However, further analysis and parameter tuning may be necessary,
especially for the DBSCAN clustering, to improve the clustering quality.
#Silhouette Score for K-Medoids Clustering: 0.4468315848864226
#Silhouette Score for DBSCAN Clustering: 0.06815381140609145
"""